:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-9fb7e9b9-636a-40ce-b320-f94b1c41df00;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central
	found org.apache.kafka#kafka-clients;2.4.1 in central
	found com.github.luben#zstd-jni;1.4.4-3 in central
	found org.lz4#lz4-java;1.7.1 in central
	found org.xerial.snappy#snappy-java;1.1.7.5 in central
	found org.slf4j#slf4j-api;1.7.30 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.commons#commons-pool2;2.6.2 in central
:: resolution report :: resolve 1308ms :: artifacts dl 139ms
	:: modules in use:
	com.github.luben#zstd-jni;1.4.4-3 from central in [default]
	org.apache.commons#commons-pool2;2.6.2 from central in [default]
	org.apache.kafka#kafka-clients;2.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]
	org.lz4#lz4-java;1.7.1 from central in [default]
	org.slf4j#slf4j-api;1.7.30 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.7.5 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-9fb7e9b9-636a-40ce-b320-f94b1c41df00
	confs: [default]
	0 artifacts copied, 9 already retrieved (0kB/40ms)
21/11/07 20:46:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/11/07 20:46:11 INFO SparkContext: Running Spark version 3.2.0
21/11/07 20:46:11 INFO ResourceUtils: ==============================================================
21/11/07 20:46:11 INFO ResourceUtils: No custom resources configured for spark.driver.
21/11/07 20:46:11 INFO ResourceUtils: ==============================================================
21/11/07 20:46:11 INFO SparkContext: Submitted application: customer-data
21/11/07 20:46:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
21/11/07 20:46:11 INFO ResourceProfile: Limiting resource is cpu
21/11/07 20:46:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
21/11/07 20:46:11 INFO SecurityManager: Changing view acls to: spark
21/11/07 20:46:11 INFO SecurityManager: Changing modify acls to: spark
21/11/07 20:46:11 INFO SecurityManager: Changing view acls groups to: 
21/11/07 20:46:11 INFO SecurityManager: Changing modify acls groups to: 
21/11/07 20:46:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
21/11/07 20:46:12 INFO Utils: Successfully started service 'sparkDriver' on port 43753.
21/11/07 20:46:12 INFO SparkEnv: Registering MapOutputTracker
21/11/07 20:46:12 INFO SparkEnv: Registering BlockManagerMaster
21/11/07 20:46:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/11/07 20:46:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/11/07 20:46:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/11/07 20:46:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3e8a7376-eeed-432c-a24f-439a80aba686
21/11/07 20:46:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
21/11/07 20:46:13 INFO SparkEnv: Registering OutputCommitCoordinator
21/11/07 20:46:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/11/07 20:46:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://a4d3b2612f22:4040
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar at spark://a4d3b2612f22:43753/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar at spark://a4d3b2612f22:43753/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar at spark://a4d3b2612f22:43753/jars/org.apache.kafka_kafka-clients-2.4.1.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://a4d3b2612f22:43753/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a4d3b2612f22:43753/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar at spark://a4d3b2612f22:43753/jars/com.github.luben_zstd-jni-1.4.4-3.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://a4d3b2612f22:43753/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at spark://a4d3b2612f22:43753/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://a4d3b2612f22:43753/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.kafka_kafka-clients-2.4.1.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.commons_commons-pool2-2.6.2.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.spark-project.spark_unused-1.0.0.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar at file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/com.github.luben_zstd-jni-1.4.4-3.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.lz4_lz4-java-1.7.1.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.xerial.snappy_snappy-java-1.1.7.5.jar
21/11/07 20:46:14 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.slf4j_slf4j-api-1.7.30.jar
21/11/07 20:46:14 INFO Executor: Starting executor ID driver on host a4d3b2612f22
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.github.luben_zstd-jni-1.4.4-3.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/com.github.luben_zstd-jni-1.4.4-3.jar
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.xerial.snappy_snappy-java-1.1.7.5.jar
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-2.4.1.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.kafka_kafka-clients-2.4.1.jar
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.commons_commons-pool2-2.6.2.jar
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.spark-project.spark_unused-1.0.0.jar
21/11/07 20:46:14 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1636317971686
21/11/07 20:46:14 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.lz4_lz4-java-1.7.1.jar
21/11/07 20:46:15 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.slf4j_slf4j-api-1.7.30.jar
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO TransportClientFactory: Successfully created connection to a4d3b2612f22/172.20.0.4:43753 after 75 ms (0 ms spent in bootstraps)
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp4614817889202085938.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp4614817889202085938.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.slf4j_slf4j-api-1.7.30.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.slf4j_slf4j-api-1.7.30.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp4587101640556825841.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp4587101640556825841.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.lz4_lz4-java-1.7.1.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.lz4_lz4-java-1.7.1.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp2280785143607777962.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp2280785143607777962.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.commons_commons-pool2-2.6.2.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.commons_commons-pool2-2.6.2.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/com.github.luben_zstd-jni-1.4.4-3.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/com.github.luben_zstd-jni-1.4.4-3.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp1809638782811673204.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp1809638782811673204.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/com.github.luben_zstd-jni-1.4.4-3.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/com.github.luben_zstd-jni-1.4.4-3.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.xerial.snappy_snappy-java-1.1.7.5.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp7259198849395439456.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp7259198849395439456.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.xerial.snappy_snappy-java-1.1.7.5.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.xerial.snappy_snappy-java-1.1.7.5.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.apache.kafka_kafka-clients-2.4.1.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.apache.kafka_kafka-clients-2.4.1.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp6932567608612486333.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp6932567608612486333.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.kafka_kafka-clients-2.4.1.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.kafka_kafka-clients-2.4.1.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp2456694531579962203.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp2456694531579962203.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.0.0.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp7407436266081234104.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp7407436266081234104.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.spark-project.spark_unused-1.0.0.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.spark-project.spark_unused-1.0.0.jar to class loader
21/11/07 20:46:15 INFO Executor: Fetching spark://a4d3b2612f22:43753/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar with timestamp 1636317971686
21/11/07 20:46:15 INFO Utils: Fetching spark://a4d3b2612f22:43753/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp6634416736677961031.tmp
21/11/07 20:46:15 INFO Utils: /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/fetchFileTemp6634416736677961031.tmp has been previously copied to /tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar
21/11/07 20:46:15 INFO Executor: Adding file:/tmp/spark-bee9dda2-3164-4c20-a13e-4059b4ca3d44/userFiles-2f217068-14e2-4426-8e01-1eb7dde8a093/org.apache.spark_spark-sql-kafka-0-10_2.12-3.0.0.jar to class loader
21/11/07 20:46:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41357.
21/11/07 20:46:15 INFO NettyBlockTransferService: Server created on a4d3b2612f22:41357
21/11/07 20:46:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/11/07 20:46:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a4d3b2612f22, 41357, None)
21/11/07 20:46:15 INFO BlockManagerMasterEndpoint: Registering block manager a4d3b2612f22:41357 with 366.3 MiB RAM, BlockManagerId(driver, a4d3b2612f22, 41357, None)
21/11/07 20:46:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a4d3b2612f22, 41357, None)
21/11/07 20:46:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a4d3b2612f22, 41357, None)
/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.
  FutureWarning
21/11/07 20:46:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
21/11/07 20:46:16 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
21/11/07 20:46:24 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5792a125-a6fa-43d2-ba15-016fe13e82b3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
21/11/07 20:46:24 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------------+---------+
|               email|birthYear|
+--------------------+---------+
|David.Harris@test...|     1965|
|Trevor.Huey@test.com|     1964|
|David.Sanchez@tes...|     1963|
|   Angie.Wu@test.com|     1962|
|Frank.Sanchez@tes...|     1961|
|Sarah.Abram@test.com|     1960|
|Danny.Habschied@t...|     1959|
|Jason.Ahmed@test.com|     1958|
|Larry.Anandh@test...|     1957|
|John.Habschied@te...|     1956|
|Frank.Jones@test.com|     1955|
|Jason.Habschied@t...|     1953|
|Ashley.Jackson@te...|     1952|
| John.Davis@test.com|     1950|
|Larry.Anderson@te...|     1949|
|Jerry.Howard@test...|     1948|
|Gail.Staples@test...|     1947|
|Frank.Phillips@te...|     1946|
|Larry.Gonzalez@te...|     1945|
|Sarah.Lincoln@tes...|     1944|
+--------------------+---------+
only showing top 20 rows

-------------------------------------------
Batch: 1
-------------------------------------------
+-----+---------+
|email|birthYear|
+-----+---------+
+-----+---------+

^CERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py", line 475, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py", line 504, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py", line 475, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.6/socket.py", line 586, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 292, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 1195, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1310, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py", line 336, in get_return_value
    format(target_id, ".", name))
py4j.protocol.Py4JError: An error occurred while calling o14.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/clientserver.py", line 504, in send_command
    "Error while sending or receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while sending or receiving
Traceback (most recent call last):
  File "/home/workspace/project/starter/sparkpyrediskafkastreamtoconsole.py", line 57, in <module>
    emailAndBirthYearStreamingDF.writeStream.format("console").outputMode("append").start().awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 101, in awaitTermination
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py", line 1310, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py", line 336, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o72.awaitTermination
21/11/07 20:46:37 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@18488aeb is aborting.
21/11/07 20:46:37 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@18488aeb aborted.
21/11/07 20:46:37 ERROR MicroBatchExecution: Query [id = 075de146-aa79-4597-bf3d-bcd434540835, runId = 167beaa9-08f3-4346-a964-1a52a0464210] terminated with error
org.apache.spark.SparkException: Writing job aborted
	at org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobAbortedError(QueryExecutionErrors.scala:613)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:386)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:330)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:279)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:290)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)
	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2971)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2971)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:603)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.lang.Thread.run(Thread.java:748)

The currently active SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.lang.Thread.run(Thread.java:748)
         
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.lang.Thread.run(Thread.java:748)

The currently active SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.lang.Thread.run(Thread.java:748)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1512)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1427)
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1198)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2541)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1198)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2541)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:354)
	... 40 more
Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.lang.Thread.run(Thread.java:748)

The currently active SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.lang.Thread.run(Thread.java:748)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1512)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1427)
	at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1198)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2541)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
21/11/07 20:46:37 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@18488005 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@63d942e5[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]
Exception in thread "stream execution thread for [id = 075de146-aa79-4597-bf3d-bcd434540835, runId = 167beaa9-08f3-4346-a964-1a52a0464210]" org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
	at org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:119)
	at org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:402)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$3(StreamExecution.scala:352)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:333)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)
Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)
	... 8 more
